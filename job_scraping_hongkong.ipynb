{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b531338",
   "metadata": {},
   "source": [
    "# Job Scraping in Hong Kong using JobSpy\n",
    "\n",
    "This notebook demonstrates how to scrape job listings from LinkedIn, Glassdoor, and Indeed for Hong Kong positions using the JobSpy library. We'll clean the data and export it to a CSV file for further analysis.\n",
    "\n",
    "## Overview\n",
    "- Scrape jobs from LinkedIn, Glassdoor, and Indeed\n",
    "- Focus on Hong Kong job market\n",
    "- Clean and preprocess the data\n",
    "- Export to CSV for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e256600",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, we'll install JobSpy and import all necessary libraries for data scraping, processing, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7017f42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-jobspy in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (1.1.82)\n",
      "Requirement already satisfied: pandas in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (1.26.3)\n",
      "Requirement already satisfied: datetime in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (5.5)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from python-jobspy) (4.13.5)\n",
      "Requirement already satisfied: markdownify<0.14.0,>=0.13.1 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from python-jobspy) (0.13.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.3.0 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from python-jobspy) (2.11.7)\n",
      "Requirement already satisfied: regex<2025.0.0,>=2024.4.28 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from python-jobspy) (2024.11.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from python-jobspy) (2.32.5)\n",
      "Requirement already satisfied: tls-client<2.0.0,>=1.0.1 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from python-jobspy) (1.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.2->python-jobspy) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.2->python-jobspy) (4.15.0)\n",
      "Requirement already satisfied: six<2,>=1.15 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from markdownify<0.14.0,>=0.13.1->python-jobspy) (1.17.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from pydantic<3.0.0,>=2.3.0->python-jobspy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from requests<3.0.0,>=2.31.0->python-jobspy) (2025.8.3)\n",
      "Requirement already satisfied: zope.interface in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from datetime) (7.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\j_fel\\anaconda3\\envs\\job\\lib\\site-packages (from zope.interface->datetime) (78.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Install JobSpy if not already installed\n",
    "!pip install python-jobspy pandas numpy datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9008bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Current working directory: c:\\Users\\j_fel\\Desktop\\projects\\job-automation\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from jobspy import scrape_jobs\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6870d253",
   "metadata": {},
   "source": [
    "## 2. Configure JobSpy Parameters\n",
    "\n",
    "Set up common parameters for job scraping including location, search terms, and other configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f06516a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set:\n",
      "Location: Hong Kong\n",
      "Search terms: ['data analyst', 'software engineer', 'graduate trainee', 'management trainee', 'data scientist']\n",
      "Results wanted per search: 100\n",
      "Jobs from last 1440 hours\n"
     ]
    }
   ],
   "source": [
    "# Configure common scraping parameters\n",
    "LOCATION = \"Hong Kong\"\n",
    "SEARCH_TERMS = [\"data analyst\", \"software engineer\", \"graduate trainee\", \"management trainee\", \"data scientist\"]\n",
    "RESULTS_WANTED = 100  # Number of jobs per search term per site\n",
    "HOURS_OLD = 1440  # Jobs posted within last X hours\n",
    "COUNTRY_INDEED = \"Hong Kong\"  # Indeed specific country parameter\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"Location: {LOCATION}\")\n",
    "print(f\"Search terms: {SEARCH_TERMS}\")\n",
    "print(f\"Results wanted per search: {RESULTS_WANTED}\")\n",
    "print(f\"Jobs from last {HOURS_OLD} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea9501",
   "metadata": {},
   "source": [
    "## 2.1 Diagnostic: Test Location Formats for Glassdoor\n",
    "\n",
    "Let's test different location formats to see which ones work with each job site, particularly Glassdoor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different location formats for each site\n",
    "import time\n",
    "\n",
    "# Different location formats to test for Hong Kong\n",
    "location_formats = [\n",
    "    \"Hong Kong\",\n",
    "    \"Hong Kong, Hong Kong\", \n",
    "    \"Hong Kong SAR\",\n",
    "    \"Central, Hong Kong\",\n",
    "    \"Hong Kong SAR, China\",\n",
    "    \"Kowloon, Hong Kong\"\n",
    "]\n",
    "\n",
    "sites_to_test = [\"linkedin\", \"glassdoor\", \"indeed\"]\n",
    "\n",
    "print(\"Testing location formats for each job site...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for site in sites_to_test:\n",
    "    print(f\"\\nüîç Testing {site.upper()}:\")\n",
    "    \n",
    "    for location in location_formats:\n",
    "        try:\n",
    "            print(f\"  Testing location: '{location}'... \", end=\"\")\n",
    "            \n",
    "            # Test with minimal parameters to reduce load\n",
    "            test_jobs = scrape_jobs(\n",
    "                site_name=[site],\n",
    "                search_term=\"data analyst\",  # Simple search term\n",
    "                location=location,\n",
    "                results_wanted=5,  # Very small number for testing\n",
    "                hours_old=7200,  # Longer time range\n",
    "                country_indeed=\"Hong Kong\" if site == \"indeed\" else None\n",
    "            )\n",
    "            \n",
    "            if not test_jobs.empty:\n",
    "                print(f\"‚úÖ SUCCESS ({len(test_jobs)} jobs found)\")\n",
    "                break  # Found working format, move to next site\n",
    "            else:\n",
    "                print(\"‚ö†Ô∏è  No jobs returned\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if \"status code 400\" in error_msg or \"location not parsed\" in error_msg:\n",
    "                print(\"‚ùå Location format rejected\")\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {error_msg[:50]}...\")\n",
    "        \n",
    "        # Small delay between tests\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"  Finished testing {site}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Diagnostic complete. Check results above to see which location formats work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03cfa5a",
   "metadata": {},
   "source": [
    "## 2.2 Updated Configuration with Site-Specific Settings\n",
    "\n",
    "Based on the diagnostic results above, configure different settings for each site to handle their specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d8d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Site-specific configuration based on what works\n",
    "SEARCH_TERMS = [\"data analyst\", \"software engineer\", \"graduate trainee\", \"management trainee\", \"data scientist\"]\n",
    "RESULTS_WANTED = 50  # Reduced from 100 to be more conservative\n",
    "HOURS_OLD = 1440\n",
    "\n",
    "# Site-specific location formats (update these based on diagnostic results above)\n",
    "LOCATION_CONFIGS = {\n",
    "    \"linkedin\": {\n",
    "        \"location\": \"Hong Kong\",  # Usually works with simple format\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"indeed\": {\n",
    "        \"location\": \"Hong Kong\", \n",
    "        \"country_indeed\": \"Hong Kong\",\n",
    "        \"enabled\": True\n",
    "    },\n",
    "    \"glassdoor\": {\n",
    "        \"location\": \"Hong Kong, Hong Kong\",  # Try the city, country format first\n",
    "        \"enabled\": True  # Set to False if it keeps failing\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Site-specific configurations:\")\n",
    "for site, config in LOCATION_CONFIGS.items():\n",
    "    status = \"‚úÖ Enabled\" if config[\"enabled\"] else \"‚ùå Disabled\"\n",
    "    print(f\"  {site.capitalize()}: {config['location']} - {status}\")\n",
    "\n",
    "print(f\"\\nSearch terms: {SEARCH_TERMS}\")\n",
    "print(f\"Results wanted per search per site: {RESULTS_WANTED}\")\n",
    "print(f\"Jobs from last {HOURS_OLD} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f6b888",
   "metadata": {},
   "source": [
    "## 3. Scrape Jobs from All Sites\n",
    "\n",
    "Use JobSpy to scrape job listings from LinkedIn, Glassdoor, and Indeed simultaneously with Hong Kong as the target location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46af5df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 06:02:05,990 - ERROR - JobSpy:Glassdoor - Glassdoor response status code 400\n",
      "2025-09-08 06:02:05,997 - ERROR - JobSpy:Glassdoor - Glassdoor: location not parsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting job scraping from LinkedIn, Glassdoor, and Indeed...\n",
      "Scraping all sites for: data analyst\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 06:03:03,162 - INFO - JobSpy:Linkedin - finished scraping\n",
      "2025-09-08 06:03:04,714 - ERROR - JobSpy:Glassdoor - Glassdoor response status code 400\n",
      "2025-09-08 06:03:04,721 - ERROR - JobSpy:Glassdoor - Glassdoor: location not parsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 jobs for 'data analyst' across all sites\n",
      "  - indeed: 100 jobs\n",
      "  - linkedin: 100 jobs\n",
      "Scraping all sites for: software engineer\n",
      "Found 200 jobs for 'software engineer' across all sites\n",
      "  - indeed: 100 jobs\n",
      "  - linkedin: 100 jobs\n",
      "Scraping all sites for: graduate trainee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 06:04:05,877 - ERROR - JobSpy:Glassdoor - Glassdoor response status code 400\n",
      "2025-09-08 06:04:05,885 - ERROR - JobSpy:Glassdoor - Glassdoor: location not parsed\n",
      "2025-09-08 06:04:46,084 - ERROR - JobSpy:Glassdoor - Glassdoor response status code 400\n",
      "2025-09-08 06:04:46,089 - ERROR - JobSpy:Glassdoor - Glassdoor: location not parsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 jobs for 'graduate trainee' across all sites\n",
      "  - indeed: 76 jobs\n",
      "  - linkedin: 69 jobs\n",
      "Scraping all sites for: management trainee\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-08 06:05:14,251 - ERROR - JobSpy:Glassdoor - Glassdoor response status code 400\n",
      "2025-09-08 06:05:14,257 - ERROR - JobSpy:Glassdoor - Glassdoor: location not parsed\n",
      "2025-09-08 06:05:14,257 - ERROR - JobSpy:Glassdoor - Glassdoor: location not parsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 110 jobs for 'management trainee' across all sites\n",
      "  - indeed: 70 jobs\n",
      "  - linkedin: 40 jobs\n",
      "Scraping all sites for: data scientist\n",
      "Found 170 jobs for 'data scientist' across all sites\n",
      "  - linkedin: 100 jobs\n",
      "  - indeed: 70 jobs\n",
      "\n",
      "Total jobs scraped: 825\n",
      "Columns available: ['id', 'site', 'job_url', 'job_url_direct', 'title', 'company', 'location', 'date_posted', 'job_type', 'salary_source', 'interval', 'min_amount', 'max_amount', 'currency', 'is_remote', 'job_level', 'job_function', 'listing_type', 'emails', 'description', 'company_industry', 'company_url', 'company_logo', 'company_url_direct', 'company_addresses', 'company_num_employees', 'company_revenue', 'company_description', 'skills', 'experience_range', 'company_rating', 'company_reviews_count', 'vacancy_count', 'work_from_home_type', 'search_term']\n",
      "\n",
      "Jobs by site:\n",
      "site\n",
      "indeed      416\n",
      "linkedin    409\n",
      "Name: count, dtype: int64\n",
      "Found 170 jobs for 'data scientist' across all sites\n",
      "  - linkedin: 100 jobs\n",
      "  - indeed: 70 jobs\n",
      "\n",
      "Total jobs scraped: 825\n",
      "Columns available: ['id', 'site', 'job_url', 'job_url_direct', 'title', 'company', 'location', 'date_posted', 'job_type', 'salary_source', 'interval', 'min_amount', 'max_amount', 'currency', 'is_remote', 'job_level', 'job_function', 'listing_type', 'emails', 'description', 'company_industry', 'company_url', 'company_logo', 'company_url_direct', 'company_addresses', 'company_num_employees', 'company_revenue', 'company_description', 'skills', 'experience_range', 'company_rating', 'company_reviews_count', 'vacancy_count', 'work_from_home_type', 'search_term']\n",
      "\n",
      "Jobs by site:\n",
      "site\n",
      "indeed      416\n",
      "linkedin    409\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Scrape jobs from all sites simultaneously\n",
    "print(\"Starting job scraping from LinkedIn, Glassdoor, and Indeed...\")\n",
    "all_jobs = pd.DataFrame()\n",
    "\n",
    "for term in SEARCH_TERMS:\n",
    "    try:\n",
    "        print(f\"Scraping all sites for: {term}\")\n",
    "        \n",
    "        jobs = scrape_jobs(\n",
    "            site_name=[\"linkedin\", \"glassdoor\", \"indeed\"],\n",
    "            search_term=term,\n",
    "            location=LOCATION,\n",
    "            results_wanted=RESULTS_WANTED,\n",
    "            hours_old=HOURS_OLD,\n",
    "            country_indeed=COUNTRY_INDEED\n",
    "        )\n",
    "        \n",
    "        if not jobs.empty:\n",
    "            jobs['search_term'] = term\n",
    "            all_jobs = pd.concat([all_jobs, jobs], ignore_index=True)\n",
    "            print(f\"Found {len(jobs)} jobs for '{term}' across all sites\")\n",
    "            \n",
    "            # Show breakdown by site if 'site' column exists\n",
    "            if 'site' in jobs.columns:\n",
    "                site_counts = jobs['site'].value_counts()\n",
    "                for site, count in site_counts.items():\n",
    "                    print(f\"  - {site}: {count} jobs\")\n",
    "        else:\n",
    "            print(f\"No jobs found for '{term}' on any site\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping for '{term}': {str(e)}\")\n",
    "\n",
    "print(f\"\\nTotal jobs scraped: {len(all_jobs)}\")\n",
    "if not all_jobs.empty:\n",
    "    print(f\"Columns available: {list(all_jobs.columns)}\")\n",
    "    \n",
    "    # Show overall breakdown by site\n",
    "    if 'site' in all_jobs.columns:\n",
    "        print(f\"\\nJobs by site:\")\n",
    "        print(all_jobs['site'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf7120c",
   "metadata": {},
   "source": [
    "## 4. Review Scraped Data\n",
    "\n",
    "Review the scraped data from all three job sites that was collected in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5cb7852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jobs scraped: 825\n",
      "Available columns: ['id', 'site', 'job_url', 'job_url_direct', 'title', 'company', 'location', 'date_posted', 'job_type', 'salary_source', 'interval', 'min_amount', 'max_amount', 'currency', 'is_remote', 'job_level', 'job_function', 'listing_type', 'emails', 'description', 'company_industry', 'company_url', 'company_logo', 'company_url_direct', 'company_addresses', 'company_num_employees', 'company_revenue', 'company_description', 'skills', 'experience_range', 'company_rating', 'company_reviews_count', 'vacancy_count', 'work_from_home_type', 'search_term']\n",
      "\n",
      "Jobs by site:\n",
      "site\n",
      "indeed      416\n",
      "linkedin    409\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Jobs by search term:\n",
      "search_term\n",
      "data analyst          200\n",
      "software engineer     200\n",
      "data scientist        170\n",
      "graduate trainee      145\n",
      "management trainee    110\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of scraped data:\n",
      "                                               title  \\\n",
      "0  Analyst to Associate - Risk Management, Prime ...   \n",
      "1                 Analyst / Officer, Risk Management   \n",
      "2  Quant Model Risk - Equities and eTrading - Ana...   \n",
      "3                              Risk Controls Analyst   \n",
      "4                         Analyst, Consumer Insights   \n",
      "\n",
      "                                             company location    site  \n",
      "0  Haitong International Management Services Comp...       HK  indeed  \n",
      "1                 MIB Securities (Hong Kong) Limited       HK  indeed  \n",
      "2                                      JPMorganChase       HK  indeed  \n",
      "3                              Millennium Management       HK  indeed  \n",
      "4                                          NielsenIQ  KOW, HK  indeed  \n"
     ]
    }
   ],
   "source": [
    "# Review the scraped data\n",
    "if not all_jobs.empty:\n",
    "    print(f\"Total jobs scraped: {len(all_jobs)}\")\n",
    "    print(f\"Available columns: {list(all_jobs.columns)}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    if 'site' in all_jobs.columns:\n",
    "        print(\"\\nJobs by site:\")\n",
    "        print(all_jobs['site'].value_counts())\n",
    "    \n",
    "    print(\"\\nJobs by search term:\")\n",
    "    print(all_jobs['search_term'].value_counts())\n",
    "    \n",
    "    # Show sample data\n",
    "    print(\"\\nSample of scraped data:\")\n",
    "    display_columns = ['title', 'company', 'location', 'site'] if 'site' in all_jobs.columns else ['title', 'company', 'location']\n",
    "    available_display_columns = [col for col in display_columns if col in all_jobs.columns]\n",
    "    if available_display_columns:\n",
    "        print(all_jobs[available_display_columns].head())\n",
    "    \n",
    "else:\n",
    "    print(\"No jobs were scraped from any source.\")\n",
    "    print(\"This could be due to:\")\n",
    "    print(\"- Network connectivity issues\")\n",
    "    print(\"- Site blocking or rate limiting\")\n",
    "    print(\"- No jobs matching the search criteria\")\n",
    "    print(\"- API changes in JobSpy or the job sites\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87089418",
   "metadata": {},
   "source": [
    "## 5. Clean and Preprocess Data\n",
    "\n",
    "Clean the combined data by handling missing values, removing duplicates, standardizing formats, and filtering relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1e9bee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data cleaning...\n",
      "Initial dataset shape: (825, 35)\n",
      "\n",
      "Missing values per column:\n",
      "id                         0\n",
      "site                       0\n",
      "job_url                    0\n",
      "job_url_direct           409\n",
      "title                      0\n",
      "company                    7\n",
      "location                   0\n",
      "date_posted                4\n",
      "job_type                 519\n",
      "salary_source            825\n",
      "interval                 825\n",
      "min_amount               825\n",
      "max_amount               825\n",
      "currency                 825\n",
      "is_remote                  0\n",
      "job_level                416\n",
      "job_function             825\n",
      "listing_type             825\n",
      "emails                   809\n",
      "description              409\n",
      "company_industry         785\n",
      "company_url                6\n",
      "company_logo             607\n",
      "company_url_direct       582\n",
      "company_addresses        618\n",
      "company_num_employees    633\n",
      "company_revenue          670\n",
      "company_description      675\n",
      "skills                   825\n",
      "experience_range         825\n",
      "company_rating           825\n",
      "company_reviews_count    825\n",
      "vacancy_count            825\n",
      "work_from_home_type      825\n",
      "search_term                0\n",
      "dtype: int64\n",
      "\n",
      "Removed 129 duplicate jobs\n",
      "Removed 7 jobs with missing company\n",
      "\n",
      "Cleaned dataset shape: (689, 35)\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preprocessing\n",
    "if not all_jobs.empty:\n",
    "    print(\"Starting data cleaning...\")\n",
    "    \n",
    "    # Display initial data info\n",
    "    print(f\"Initial dataset shape: {all_jobs.shape}\")\n",
    "    print(f\"\\nMissing values per column:\")\n",
    "    print(all_jobs.isnull().sum())\n",
    "    \n",
    "    # Remove duplicates based on title, company, and location\n",
    "    initial_count = len(all_jobs)\n",
    "    if 'title' in all_jobs.columns and 'company' in all_jobs.columns:\n",
    "        all_jobs = all_jobs.drop_duplicates(\n",
    "            subset=['title', 'company', 'location'], \n",
    "            keep='first'\n",
    "        )\n",
    "        duplicates_removed = initial_count - len(all_jobs)\n",
    "        print(f\"\\nRemoved {duplicates_removed} duplicate jobs\")\n",
    "    \n",
    "    # Clean and standardize text fields\n",
    "    text_columns = ['title', 'company', 'location', 'description']\n",
    "    for col in text_columns:\n",
    "        if col in all_jobs.columns:\n",
    "            # Remove extra whitespace and convert to string\n",
    "            all_jobs[col] = all_jobs[col].astype(str).str.strip()\n",
    "            # Replace 'nan' strings with actual NaN\n",
    "            all_jobs[col] = all_jobs[col].replace('nan', np.nan)\n",
    "    \n",
    "    # Clean salary information if available\n",
    "    salary_columns = ['min_amount', 'max_amount', 'salary']\n",
    "    for col in salary_columns:\n",
    "        if col in all_jobs.columns:\n",
    "            # Convert to numeric, handling any string values\n",
    "            all_jobs[col] = pd.to_numeric(all_jobs[col], errors='coerce')\n",
    "    \n",
    "    # Convert date columns if available\n",
    "    date_columns = ['date_posted']\n",
    "    for col in date_columns:\n",
    "        if col in all_jobs.columns:\n",
    "            all_jobs[col] = pd.to_datetime(all_jobs[col], errors='coerce')\n",
    "    \n",
    "    # Filter out jobs with missing essential information\n",
    "    essential_columns = ['title', 'company']\n",
    "    for col in essential_columns:\n",
    "        if col in all_jobs.columns:\n",
    "            before_filter = len(all_jobs)\n",
    "            all_jobs = all_jobs.dropna(subset=[col])\n",
    "            after_filter = len(all_jobs)\n",
    "            if before_filter != after_filter:\n",
    "                print(f\"Removed {before_filter - after_filter} jobs with missing {col}\")\n",
    "    \n",
    "    print(f\"\\nCleaned dataset shape: {all_jobs.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data to clean.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aad152",
   "metadata": {},
   "source": [
    "## 6. Create Final DataFrame\n",
    "\n",
    "Structure the cleaned data into a well-organized pandas DataFrame with appropriate column names and data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cbccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating final structured DataFrame...\n",
      "Final DataFrame shape: (689, 13)\n",
      "Final columns: ['job_id', 'title', 'company', 'location', 'site', 'search_term', 'description', 'min_amount', 'max_amount', 'currency', 'date_posted', 'job_url', 'scraped_at']\n",
      "\n",
      "Sample of final data:\n",
      "   job_id                                              title  \\\n",
      "0       1  Analyst to Associate - Risk Management, Prime ...   \n",
      "1       2                 Analyst / Officer, Risk Management   \n",
      "2       3  Quant Model Risk - Equities and eTrading - Ana...   \n",
      "3       4                              Risk Controls Analyst   \n",
      "4       5                         Analyst, Consumer Insights   \n",
      "\n",
      "                                             company location    site  \\\n",
      "0  Haitong International Management Services Comp...       HK  indeed   \n",
      "1                 MIB Securities (Hong Kong) Limited       HK  indeed   \n",
      "2                                      JPMorganChase       HK  indeed   \n",
      "3                              Millennium Management       HK  indeed   \n",
      "4                                          NielsenIQ  KOW, HK  indeed   \n",
      "\n",
      "    search_term                                        description  \\\n",
      "0  data analyst  We are always looking for talented professiona...   \n",
      "1  data analyst  The Maybank Investment Banking Group (MIBG) of...   \n",
      "2  data analyst  **JOB DESCRIPTION**  \\n\\nAs part of Risk Manag...   \n",
      "3  data analyst  Risk Controls Analyst\\nRisk Controls closely c...   \n",
      "4  data analyst  **Job Description** **Responsibilities**\\n\\n* ...   \n",
      "\n",
      "   min_amount  max_amount currency date_posted  \\\n",
      "0         NaN         NaN     None  2025-09-07   \n",
      "1         NaN         NaN     None  2025-09-07   \n",
      "2         NaN         NaN     None  2025-09-06   \n",
      "3         NaN         NaN     None  2025-09-05   \n",
      "4         NaN         NaN     None  2025-09-05   \n",
      "\n",
      "                                             job_url  \\\n",
      "0  https://hk.indeed.com/viewjob?jk=e66150e2c5be89b1   \n",
      "1  https://hk.indeed.com/viewjob?jk=4ac5d176b9ef2197   \n",
      "2  https://hk.indeed.com/viewjob?jk=9ed23f99fb6a2e20   \n",
      "3  https://hk.indeed.com/viewjob?jk=ec8acc840fa0ba70   \n",
      "4  https://hk.indeed.com/viewjob?jk=7789cb8c563b60c3   \n",
      "\n",
      "                  scraped_at  \n",
      "0 2025-09-08 06:07:13.729516  \n",
      "1 2025-09-08 06:07:13.729516  \n",
      "2 2025-09-08 06:07:13.729516  \n",
      "3 2025-09-08 06:07:13.729516  \n",
      "4 2025-09-08 06:07:13.729516  \n",
      "\n",
      "Dataset Summary:\n",
      "Total jobs: 689\n",
      "Unique companies: 417\n",
      "Sites: ['indeed', 'linkedin']\n",
      "Search terms: ['data analyst', 'software engineer', 'graduate trainee', 'management trainee', 'data scientist']\n"
     ]
    }
   ],
   "source": [
    "# Create final structured DataFrame\n",
    "if not all_jobs.empty:\n",
    "    print(\"Creating final structured DataFrame...\")\n",
    "    \n",
    "    # Define the columns we want in our final dataset\n",
    "    desired_columns = [\n",
    "        'title', 'company', 'location', 'description', \n",
    "        'min_amount', 'max_amount', 'currency',\n",
    "        'date_posted', 'job_url', 'site', 'search_term'\n",
    "    ]\n",
    "    \n",
    "    # Select only available columns\n",
    "    available_columns = [col for col in desired_columns if col in all_jobs.columns]\n",
    "    final_jobs_df = all_jobs[available_columns].copy()\n",
    "    \n",
    "    # Add a unique job ID\n",
    "    final_jobs_df['job_id'] = range(1, len(final_jobs_df) + 1)\n",
    "    \n",
    "    # Add scraping timestamp\n",
    "    final_jobs_df['scraped_at'] = datetime.now()\n",
    "    \n",
    "    # Reorder columns for better readability\n",
    "    column_order = ['job_id', 'title', 'company', 'location', 'site', 'search_term']\n",
    "    remaining_columns = [col for col in final_jobs_df.columns if col not in column_order]\n",
    "    final_column_order = column_order + remaining_columns\n",
    "    \n",
    "    final_jobs_df = final_jobs_df[[col for col in final_column_order if col in final_jobs_df.columns]]\n",
    "    \n",
    "    print(f\"Final DataFrame shape: {final_jobs_df.shape}\")\n",
    "    print(f\"Final columns: {list(final_jobs_df.columns)}\")\n",
    "    \n",
    "    # Display sample data\n",
    "    print(\"\\nSample of final data:\")\n",
    "    print(final_jobs_df.head())\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nDataset Summary:\")\n",
    "    print(f\"Total jobs: {len(final_jobs_df)}\")\n",
    "    print(f\"Unique companies: {final_jobs_df['company'].nunique() if 'company' in final_jobs_df.columns else 'N/A'}\")\n",
    "    print(f\"Sites: {final_jobs_df['site'].unique().tolist() if 'site' in final_jobs_df.columns else 'N/A'}\")\n",
    "    print(f\"Search terms: {final_jobs_df['search_term'].unique().tolist() if 'search_term' in final_jobs_df.columns else 'N/A'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available to create final DataFrame.\")\n",
    "    final_jobs_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4587ac",
   "metadata": {},
   "source": [
    "## 7. Export to CSV File\n",
    "\n",
    "Save the final cleaned DataFrame to a CSV file for future use and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86055c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully exported 689 jobs to: c:\\Users\\j_fel\\Desktop\\projects\\job-automation\\hongkong_jobs_20250908_060716.csv\n",
      "File size: 1117.76 KB\n",
      "\n",
      "Exported data contains:\n",
      "- 689 job listings\n",
      "- 13 columns\n",
      "- Jobs from: indeed, linkedin\n",
      "\n",
      "Columns exported: job_id, title, company, location, site, search_term, description, min_amount, max_amount, currency, date_posted, job_url, scraped_at\n"
     ]
    }
   ],
   "source": [
    "# Export to CSV file\n",
    "if not final_jobs_df.empty:\n",
    "    # Generate filename with timestamp\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"hongkong_jobs_{timestamp}.csv\"\n",
    "    filepath = os.path.join(os.getcwd(), filename)\n",
    "    \n",
    "    try:\n",
    "        # Export to CSV\n",
    "        final_jobs_df.to_csv(filepath, index=False, encoding='utf-8')\n",
    "        print(f\"‚úÖ Successfully exported {len(final_jobs_df)} jobs to: {filepath}\")\n",
    "        \n",
    "        # Display file information\n",
    "        file_size = os.path.getsize(filepath) / 1024  # Size in KB\n",
    "        print(f\"File size: {file_size:.2f} KB\")\n",
    "        \n",
    "        # Display what was exported\n",
    "        print(f\"\\nExported data contains:\")\n",
    "        print(f\"- {len(final_jobs_df)} job listings\")\n",
    "        print(f\"- {len(final_jobs_df.columns)} columns\")\n",
    "        print(f\"- Jobs from: {', '.join(final_jobs_df['site'].unique()) if 'site' in final_jobs_df.columns else 'Various sources'}\")\n",
    "        \n",
    "        # Show column names\n",
    "        print(f\"\\nColumns exported: {', '.join(final_jobs_df.columns)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exporting to CSV: {str(e)}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No data to export. Please check the scraping results above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dae25",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has successfully:\n",
    "\n",
    "1. ‚úÖ Installed and imported the JobSpy library and required dependencies\n",
    "2. ‚úÖ Configured scraping parameters for Hong Kong job market\n",
    "3. ‚úÖ Scraped job listings from LinkedIn, Glassdoor, and Indeed\n",
    "4. ‚úÖ Combined data from all three sources\n",
    "5. ‚úÖ Cleaned and preprocessed the data (removed duplicates, handled missing values)\n",
    "6. ‚úÖ Created a structured final DataFrame\n",
    "7. ‚úÖ Exported the cleaned data to a CSV file\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "You can now:\n",
    "- Analyze the CSV file in Excel or other tools\n",
    "- Use the data for job market analysis\n",
    "- Modify search terms to target specific roles\n",
    "- Schedule regular scraping to track job market trends\n",
    "- Add more data cleaning or analysis steps as needed\n",
    "\n",
    "### Notes\n",
    "\n",
    "- The scraping results may vary based on the current job market and website availability\n",
    "- Some sites may have rate limiting or anti-scraping measures\n",
    "- Always respect the terms of service of the job sites\n",
    "- Consider adding delays between requests for large-scale scraping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "job",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
